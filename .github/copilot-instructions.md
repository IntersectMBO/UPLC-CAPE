# UPLC-CAPE Copilot Instructions

This document provides unified context and guidelines for GitHub Copilot and other AI assistants working on the UPLC-CAPE project.

## Project Overview

UPLC-CAPE is a standardized benchmarking framework for measuring and comparing on-chain performance of UPLC (Untyped Plutus Core) programs generated by different Cardano smart contract compilers.

**Core Principles:**

- Optional source code publishing (contributors can keep source private)
- Automation-ready structure for future tooling
- Reproducible benchmark results across different environments
- Standardized reporting with predefined templates
- Performance-first design with strict resource constraints
- Schema-validated submissions ensuring data integrity

## Development Environment

**Primary Environment:** Nix development shell with integrated tooling:

```bash
nix develop  # Enter development environment
direnv allow # Or use direnv (recommended)
```

**Available Tools:**

- `cape` - Project management CLI with comprehensive subcommands
- `measure` - UPLC performance measurement tool
- `cabal` - Haskell build tool (version 3.4+)
- `fourmolu` - Haskell formatter with project-specific configuration
- `treefmt` - Multi-language formatter (shell, markdown, Haskell, cabal, Nix)
- `adr` - Architecture Decision Record management
- `plutus` - Plutus CLI for debugging and optimization
- JSON schema validation tools with comprehensive error reporting

### VS Code terminal and agent note

- In the VS Code integrated terminal (zsh), do not run commands that modify global shell options like `set -euo pipefail`. This is known to cause hangs due to a VS Code/zsh integration issue and can break the terminal session.
- Keep `set -euo pipefail` inside project shell scripts where appropriate, but omit it from interactive commands and agent-issued terminal commands in VS Code.
- When giving interactive instructions, prefer plain commands without altering shell options (e.g., use `nix develop` and then run the needed tools).

## Version Requirements

**Plutus Core Target:** 1.1.0 (mandatory)

- Use `plcVersion110` from `PlutusCore.Version`
- GHC pragma: `{-# OPTIONS_GHC -fplugin-opt PlutusTx.Plugin:target-version=1.1.0 #-}`
- SOP (Sums of Products) encoding enabled by default for efficiency

**Package Versions (CHaP):**

- plutus-core >= 1.49.0.0
- plutus-tx >= 1.49.0.0
- plutus-ledger-api >= 1.49.0.0
- plutus-tx-plugin >= 1.49.0.0
- relude ^>= 1.2.2 (replaces Prelude for consistency)
- aeson, aeson-pretty (for JSON generation)
- flat, serialise (for UPLC serialization)

**GHC:** 9.6.6 (provided in Nix shell) **Cabal:** 3.4+ (for modern project structure)

## Plutus V3 Development (Recommended)

**Script Interface:** All V3 scripts use unified interface:

```haskell
myScript :: BuiltinData -> BuiltinUnit
myScript _ = unitval  -- Success condition
```

**Import Preferences:**

1. Primary: `PlutusLedgerApi.V3`
2. Fallback: V1/V2 only if V3 doesn't export required bindings
3. Prelude: Use Relude (via mixins), NOT standard Haskell prelude
4. PlutusTx modules: Import qualified for clarity

**Essential Functions:**

- Success return: `unitval` (BuiltinUnit constructor)
- Error handling: `check` function for validation, `error` for immediate failure
- AST measurement: `countAstNodes` from `PlutusTx.Code`
- Script size: `serialiseCompiledCode` + `BS.length`
- PIR extraction: `getPirNoAnn` for debugging
- UPLC extraction: `getPlcNoAnn` for final compilation

**Critical Pragmas:**

```haskell
{-# OPTIONS_GHC -fplugin-opt PlutusTx.Plugin:target-version=1.1.0 #-}
{-# OPTIONS_GHC -fplugin-opt PlutusTx.Plugin:conservative-optimisation #-}
{-# OPTIONS_GHC -fplugin-opt PlutusTx.Plugin:remove-trace #-}  -- For production
{-# LANGUAGE Strict #-}  -- Mandatory for predictable evaluation
```

## PlutusTx Coding Best Practices

**Evaluation Strategy:**

- Always use `Strict` language extension for predictable evaluation
- Use `~` (lazy patterns) selectively for bindings that might error if evaluated prematurely
- Place conditional bindings within `case` branches to avoid premature evaluation
- Remember: PlutusTx uses strict semantics, unlike Haskell's lazy evaluation

**Function Optimization:**

- Use `INLINEABLE` instead of `INLINE` to allow PIR/UPLC inliners to make optimal decisions
- Avoid `INLINE` pragma as it significantly increases UPLC program size
- Specialize higher-order functions into recursive functions for better performance
- Use `conservative-optimisation` plugin flag to disable size/cost-negative optimizations

**Data Encoding:**

- Prefer SOP (Sums of Products) encoding for efficiency (default in Plutus Core 1.1.0)
- Use `asData` mechanism for large objects like `ScriptContext` when only partial access needed
- Extract `asData` fields using record patterns for efficient processing
- Convert `Data` objects to proper domain types (Scott/SOP) for business logic

**Resource Management:**

- Remove `trace` messages before production deployment (`remove-trace` plugin flag)
- Use `error` for immediate, unrecoverable validation failures (functions prefixed with `unsafe`)
- Avoid creating fully-materialized intermediate results in strict evaluation
- Be mindful that most PlutusTx functions (including builtins) are strict in all arguments

**Type Safety:**

- Use `fromOpaque`/`toOpaque` for builtin type conversions, not pattern matching
- Avoid direct `fromBuiltin`/`toBuiltin` conversions
- Always specify target version in compilation: `target-version=1.1.0`

## Performance Optimization

**Profiling and Analysis:**

- Use `plutus` CLI tool for local testing and budget analysis
- Set CPU/memory budget limits with `--budget` to simulate on-chain constraints
- Profile scripts to identify resource-intensive parts before optimization
- Extract PIR with `getPirNoAnn` for readable performance debugging

**Compilation Optimization:**

- Always use latest `plutus-tx-plugin` package for newest optimizations
- Apply UPLC optimization levels: `-O1` (safe), `-O2` (aggressive, may alter semantics)
- Use `--whole-opt` for multiple input programs to perform extra optimization passes
- Consider `dump-compilation-trace` plugin flag to inspect compilation steps

**Code Patterns:**

- Rewrite succinct expressions to be more efficient in strict evaluation context
- Use wrappers like `PlutusTx.Builtins.matchList` to suspend argument evaluation
- Prefer direct recursion over higher-order functions for performance-critical code
- Minimize use of `chooseList`, `chooseData` with expensive arguments

**GHC Flags Management:**

- Disable problematic GHC optimizations: `-fno-full-laziness`
- Use project-standard GHC warnings for code quality
- Enable conservative optimization flags that preserve PlutusTx semantics

## Debugging and Testing

**Local Development:**

- Use `plutus` CLI tool for comprehensive UPLC program testing
- Check for static errors in PIR, TPLC, and UPLC programs
- Run programs locally with built-in interpreter matching on-chain behavior
- Monitor live CPU and memory resource usage during execution

**Interactive Debugging:**

- Use TUI debugger (`--debug` option) for step-by-step execution
- Debugger highlights currently evaluated UPLC expressions
- Inspect compilation trace with `dump-compilation-trace` flag
- Use PIR extraction for more readable debugging output

**Validation Workflow:**

- Test all scripts within CEK machine budget constraints
- Validate JSON outputs against provided schemas
- Ensure UPLC programs are fully-applied (no parameters)
- Verify deterministic and reproducible results across environments

**Testing Integration:**

- Use `cape test` for comprehensive project validation
- Test submission creation, validation, and measurement workflows
- Validate against all schema requirements before submission
- Ensure proper integration with CAPE CLI tooling

## Code Quality Standards

**JSON Output:**

- Generate with Aeson libraries for type safety
- Use 2-space indentation (treefmt/prettier compatible)
- Must match schema in `submissions/TEMPLATE/metrics-template.json`

**Formatting:**

- Shell scripts: shfmt (2-space indent, POSIX compliance)
- Markdown/YAML/JSON: prettier (2-space indentation)
- Haskell: fourmolu (project-specific configuration)
- Cabal files: cabal-fmt (consistent dependency formatting)
- Nix files: nixfmt (RFC 166 style for consistency)
- Run `treefmt` after modifications to ensure all formatters are applied automatically

**Code Quality Enforcement:**

- Enable comprehensive GHC warnings: `-Wall -Wcompat -Wincomplete-uni-patterns`
- Use `-Wredundant-constraints -Wmissing-export-lists -Wmissing-deriving-strategies`
- Apply `-Wpartial-fields` to catch incomplete record patterns
- Ensure all warnings are addressed before submission

**Naming Conventions:**

- Shell scripts: no spaces or special characters
- Scenarios: lowercase, hyphen-separated (e.g., "fibonacci")
- Submissions: `{compiler}_{version}_{contributor}` format

## Architecture

**Directory Structure:**

```
UPLC-CAPE/
├── scenarios/              # Benchmark specifications
│   ├── TEMPLATE/          # Templates for new benchmarks
│   └── {benchmark}.md     # Individual specs
├── submissions/           # Compiler implementations
│   ├── TEMPLATE/         # Templates for new submissions
│   └── {benchmark}/      # Organized by benchmark
│       └── {Compiler}_{version}_{contributor}/
├── scripts/              # Management tools
│   ├── cape.sh          # Main CLI
│   └── cape-subcommands/ # Command implementations
├── lib/                  # Haskell library modules
├── measure-app/          # UPLC performance measurement tool source
└── plinth-submissions-app/ # Plinth submission generator source
```

**Core Entities:**

- **Scenario**: Abstract benchmark specification with multiple implementation views
- **Submission**: Complete implementation with UPLC, metrics, metadata, and documentation
- **Compiler**: Tool version (each version treated as distinct entity for comparison)
- **Results**: Performance measurements (CPU units, memory units, script size, term size)
- **Measurement**: Automated performance analysis using CEK machine evaluation

## Common Commands

```bash
# Project Management
cape benchmark list          # List benchmarks
cape benchmark fibonacci     # View specific benchmark
cape benchmark new <name>    # Create new benchmark
cape submission list         # List submissions
cape submission new <benchmark> <compiler> <version> <handle>
cape submission verify       # Verify correctness and schemas

# Development
cabal build                 # Build Haskell code
treefmt                    # Format all files
test/test-scripts.sh       # Run test suite

# Documentation
adr new "Decision Title"    # Create Architecture Decision Record
adr preview                # Preview ADRs
```

### Measuring performance (CLI)

Preferred (repo-aware):

- Use `cape submission measure` to measure a submission directory or a single `.uplc` file via wrapper flags. This command understands the repository layout, infers the scenario, and passes the correct verifier when available. See "CLI boundaries and flag stability" for supported flags.

Low-level (single files):

- Use the `measure` executable inside the Nix shell for file-level metrics: `measure -i <program.uplc> -t <cape-tests.json> -o <metrics.json>`. This tool is submission-agnostic and is ideal for ad‑hoc measurements or external inputs.

## Contribution Workflow

1. **Fork** repository
2. **Implementation:** Develop in Nix shell (recommended)
3. **Evaluation:** Test implementation before submission
4. **Submission:** Create PR with proper structure and all required files

**Required Submission Files:**

- `{scenario}.uplc` - Fully-applied UPLC program
- `metadata.json` - Compiler/contributor info
- `metrics.json` - Performance measurements
- `README.md` - Implementation notes
- `source/` - Optional source code

## Key Constraints

- UPLC programs must be fully-applied (no parameters)
- Must execute within CEK machine budget limits
- Results must be deterministic and reproducible
- All JSON must validate against provided schemas
- Help text goes in `BANNER.md`, not `flake.nix` shellHook

## Plutus Knowledge Reference

**Language Hierarchy:**

1. Plinth (Haskell subset) → validation logic
2. Plutus IR (PIR) → high-level intermediate representation
3. Typed Plutus Core (TPLC) → low-level IR
4. Untyped Plutus Core (UPLC) → on-chain execution

**Compilation Pattern:**

```haskell
{-# OPTIONS_GHC -fplugin-opt PlutusTx.Plugin:target-version=1.1.0 #-}

compiledScript :: CompiledCode (BuiltinData -> BuiltinUnit)
compiledScript = $$(PlutusTx.compile [||myScript||])
```

**Local Evaluation and Testing:**

```haskell
import PlutusCore.Evaluation.Machine.Cek qualified as Cek
import PlutusCore.Default (DefaultFun, DefaultUni)
import UntypedPlutusCore qualified as UPLC

-- Evaluate with budget constraints
evaluateScript :: CompiledCode (BuiltinData -> BuiltinUnit) -> BuiltinData -> IO ()
evaluateScript compiled input = do
  let uplcProgram = getPlcNoAnn compiled
  let appliedProgram = UPLC.applyProgram uplcProgram (UPLC.mkConstant () input)

  -- Set realistic budget limits
  let budget = Cek.ExBudget 1000000 1000000  -- CPU, Memory

  case Cek.runCekDeBruijn Cek.defaultCekParameters budget appliedProgram of
    (Right result, remainingBudget) -> do
      putStrLn $ "Success! Remaining budget: " <> show remainingBudget
    (Left err, usedBudget) -> do
      putStrLn $ "Error: " <> show err
      putStrLn $ "Budget used: " <> show usedBudget
```

**Performance Measurement:**

```haskell
import Flat (flat)
import Data.ByteString qualified as BS
import PlutusTx.Code (countAstNodes)

measurePerformance :: CompiledCode a -> IO ()
measurePerformance compiled = do
  let uplcProgram = getPlcNoAnn compiled
  let serialized = flat uplcProgram

  putStrLn $ "Script size (bytes): " <> show (BS.length serialized)
  putStrLn $ "Term size (AST nodes): " <> show (countAstNodes compiled)

  -- Preferred CLI for repo-aware measurement: `cape submission measure`
  -- Low-level tool (in nix shell) for single files: `measure -i <uplc> -t <cape-tests.json> -o <metrics.json>`
```

## Documentation Structure

**Primary Documentation:**

- `README.md` - Comprehensive project guide and getting started
- `CLAUDE.md` - Development environment and tooling guidance
- `doc/domain-model.md` - Detailed entity relationships and framework architecture
- `scenarios/fibonacci.md` - Example benchmark specification with multiple implementation views
- `doc/adr/` - Architecture Decision Records for significant design choices

**Technical References:**

- `.github/copilot-instructions.md` - This file, comprehensive AI assistant guidance
- `scenarios/TEMPLATE/` - Template structure for new benchmark scenarios
- `submissions/TEMPLATE/` - Template structure and schema definitions for submissions
- `treefmt.toml` - Code formatting configuration for all file types
- `flake.nix` - Nix development environment specification

**External Resources:**

- [Plutus Documentation](https://plutus.cardano.intersectmbo.org/docs/) - Official Plutus development guide
- [CHaP (Cardano Haskell Packages)](https://github.com/IntersectMBO/cardano-haskell-packages) - Package repository
- [Plutus Repository](https://github.com/IntersectMBO/plutus) - Source code and examples
- [DeepWiki Plutus](https://deepwiki.com/IntersectMBO/plutus) - AI-searchable documentation

## Project Status and Capabilities

**Development Environment:** ✅ Production Ready

- Nix flake with reproducible development shell
- Complete toolchain: CAPE CLI, measure tool, Plutus CLI, formatters
- All file types supported: shell, markdown, Haskell, cabal, Nix
- Comprehensive testing framework with `cape test`
- ADR management workflow with preview and build capabilities

**Benchmarking Framework:** ✅ Operational

- Schema-validated submission structure
- Automated performance measurement with `measure` tool
- HTML report generation with charts and comparative analysis
- CSV export for data analysis and external tooling
- Template-based benchmark and submission creation

**Quality Assurance:** ✅ Comprehensive

- Multi-language code formatting with `treefmt`
- JSON schema validation for all submission files
- Comprehensive test suite covering CLI functionality
- Budget validation and UPLC execution testing
- Documentation standards with ADR workflow

**Reference Implementation:** ✅ Available

- Haskell library with Fibonacci and Two-Party Escrow examples
- Modern PlutusTx patterns and best practices
- Relude-based prelude for consistency
- Production-ready compilation settings and optimization flags

## Development Standards

**Environment and Tooling:**

- Use Nix development environment for reproducibility across team
- Follow project formatting standards (enforce with `treefmt` before commits)
- Document architectural decisions in ADRs for significant changes
- Test all submissions with comprehensive `cape test` suite before PR submission
- Validate UPLC execution with realistic budget constraints
- Always run `treefmt` after modifications to keep formatting consistent

**Code Quality:**

- Use schema validation for all JSON files
- Maintain code quality with comprehensive GHC warnings
- Remove trace messages in production builds
- Use conservative optimization to avoid semantic changes
- Help text belongs in `BANNER.md`, not `flake.nix` shellHook

**Security and Performance:**

- Validate all external inputs with proper error handling
- Follow resource-conscious programming patterns
- Document performance characteristics and trade-offs
- Ensure deterministic execution across different environments
- Measure and optimize for on-chain resource consumption

## Haskell Code Style: Language Extensions

- Prefer enabling standard and safe language extensions in the Cabal file via `default-extensions` for each component (library/executable/test).
- Avoid per-file `{-# LANGUAGE ... #-}` pragmas for these common extensions unless there is a compelling, file-local reason.
- Examples of safe/common extensions to keep in Cabal (already used in this repo):
  - ImportQualifiedPost, NamedFieldPuns, OverloadedStrings, ScopedTypeVariables, TypeApplications, DerivingStrategies, DeriveAnyClass, DeriveGeneric, DataKinds, GeneralizedNewtypeDeriving, MultiParamTypeClasses, ViewPatterns, TemplateHaskell, FlexibleContexts, FlexibleInstances, UndecidableInstances
- If a file needs a non-standard or risky extension (e.g., `TemplateHaskellQuotes`, `NoImplicitPrelude` overrides), document the rationale inline and consider moving it to Cabal only if it is broadly applicable.
- Keep warnings strict (`-Wall` etc.) and fix warnings rather than suppressing them with pragmas.

## Haskell Import Style

- Prefer using Prelude from Relude mixin without hiding common names. Avoid `import Prelude hiding (...)` unless strictly necessary.
- When name conflicts occur, prefer qualifying the conflicting module rather than hiding Prelude. Example:
  - Use `import qualified System.Exit as Exit` and call `Exit.exitWith`, while importing `ExitCode(..)` unqualified if convenient.
- Remove unnecessary dedicated imports created only due to hidden Prelude (e.g., avoid importing `exitWith` directly by qualifying `System.Exit`).
- Keep imports minimal and consistent; prefer qualified imports for large namespaces (e.g., PlutusCore qualified as PLC).

## CLI boundaries and flag stability

To avoid churn and keep tooling predictable, respect these boundaries:

- Wrapper (repo-aware): `cape submission measure`
  - Purpose: operates on submissions and directories, aware of repository layout and scenarios.
  - Stable interface: do not add or remove flags. Supported options are:
    - `-a, --all` — measure every submission under `submissions/`
    - `-i, --input FILE` and `-o, --output FILE` — single-file mode (both required)
    - Positional `PATH` — measure all `.uplc` files under the given directory (e.g., a submission folder)
    - `--verbose`, `--no-color`
  - No scenario/verifier override flags are exposed in the wrapper; it auto-infers the scenario from the path and, if present, passes the scenario verifier to the low-level tool.

- Wrapper (repo-aware): `cape submission verify`
  - Purpose: correctness verification (via `measure`) + schema validation; aware of repository layout and scenarios.
  - Stable interface: do not add or remove flags. Supported options are:
    - `-a, --all` — verify every submission under `submissions/`
    - Positional `PATH` — verify a specific submission directory
    - `--verbose`, `--no-color`
  - No scenario/verifier override flags; wrapper auto-infers and passes the scenario verifier to `measure` when available. No `--single` flag; providing a `PATH` implies single-target verification.

- Core tool (low-level): `measure` (Haskell executable)
  - Purpose: file-level measurement and verification; NOT aware of submissions.
  - Knows nothing about repository layout or submissions; it only processes UPLC programs.
  - Accepts `-i/--input <uplc>`, `-o/--output <metrics.json>`, and optional `-v/--verifier <verifier.uplc>`.
  - Emits standardized exit codes and writes raw measurements; wrapper composes final `metrics.json` with scenario and environment.

Guideline: keep the wrappers stable and submission-aware; keep the Haskell tool small, focused, and submission-agnostic. Do not introduce new wrapper flags or remove existing ones.
