# UPLC-CAPE

Comparative Artifact Performance Evaluation for UPLC programs

A standardized benchmarking framework for measuring and comparing the on-chain performance of UPLC programs generated by different Cardano smart contract compilers.

[![License: Apache 2.0](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE) [![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg)](CONTRIBUTING.md)

---

## Table of Contents

- [Overview](#overview)
- [Quick Start](#quick-start)
  - [Prerequisites](#prerequisites)
  - [Setup](#setup)
  - [Your first benchmark](#your-first-benchmark)
- [Live Performance Reports](#live-performance-reports)
- [Available Benchmarks](#available-benchmarks)
- [Usage (CLI)](#usage-cli)
  - [Core commands](#core-commands)
  - [Interactive prompts](#interactive-prompts)
- [Creating a Submission](#creating-a-submission)
- [Metrics Explained](#metrics-explained)
- [Project Structure](#project-structure)
- [Resources](#resources)
- [Version and Tooling Requirements](#version-and-tooling-requirements)
- [Development](#development)
- [Documentation (ADRs)](#documentation-adrs)
- [Contributing](#contributing)
- [License](#license)
- [Acknowledgments](#acknowledgments)

---

## Overview

UPLC-CAPE provides a structured, reproducible way for Cardano compiler authors to:

- Benchmark compiler UPLC output against standardized scenarios
- Compare results across compilers and versions
- Track optimization progress over time
- Share results with the community

Key properties:

- Consistent benchmarks and metrics (CPU units, memory units, script size, term size)
- Reproducible results with versioned scenarios and metadata
- Automation-ready structure for future tooling

---

## Quick Start

### Prerequisites

- Nix with flakes enabled
- Git

### Setup

```zsh
# Clone and enter repository
git clone https://github.com/IntersectMBO/UPLC-CAPE.git
cd UPLC-CAPE

# Enter development environment
nix develop
# Or, if using direnv (recommended)
direnv allow

# Verify CLI
scripts/cape.sh --help
# Or use the cape shim if available in PATH
cape --help
```

### Your first benchmark

```zsh
# List available benchmarks
cape benchmark list

# View a specific benchmark
cape benchmark fibonacci

# Create a submission for your compiler
cape submission new fibonacci MyCompiler 1.0.0 myhandle
```

---

## Live Performance Reports

Latest benchmark reports: [UPLC-CAPE Reports](https://intersectmbo.github.io/UPLC-CAPE/)

---

## Available Benchmarks

| Benchmark | Type | Description | Status |
| --- | --- | --- | --- |
| [Fibonacci](scenarios/fibonacci.md) | Synthetic | Recursive algorithm performance | Ready |
| Two-Party Escrow | Real-world | Smart contract scenario | Planned |
| Streaming Payments | Real-world | Payment channel implementation | Planned |
| Simple DAO Voting | Real-world | Governance mechanism | Planned |
| Time-locked Staking | Real-world | Staking protocol | Planned |

---

## Usage (CLI)

### Core commands

```zsh
# Benchmarks
cape benchmark list              # List all benchmarks
cape benchmark <name>            # Show benchmark details
cape benchmark new <name>        # Create a new benchmark from template

# Submissions
cape submission list             # List all submissions
cape submission list <name>      # List submissions for a benchmark
cape submission new <benchmark> <compiler> <version> <handle>
cape submission validate         # Validate current directory (or provide a path)
cape submission validate --all   # Validate all submissions
cape submission validate --single <file>  # Validate specific file
cape submission measure          # Measure UPLC performance
cape submission aggregate        # Generate CSV performance report
cape submission report <name>    # Generate HTML report for a benchmark
cape submission report --all     # Generate HTML reports for all benchmarks
```

### Interactive prompts

```zsh
cape benchmark new               # Prompts for benchmark name
cape submission new              # Prompts for all required fields
cape submission new fibonacci    # Prompts for compiler, version, handle
```

---

## Creating a Submission

1. Choose a benchmark

   ```zsh
   cape benchmark list
   cape benchmark fibonacci
   ```

1. Create submission structure

   ```zsh
   cape submission new fibonacci MyCompiler 1.0.0 myhandle
   # → submissions/fibonacci/MyCompiler_1.0.0_myhandle/
   ```

1. Add your UPLC program

   - Replace the placeholder UPLC with your fully-applied program (no parameters).
   - Path:
     - submissions/fibonacci/MyCompiler_1.0.0_myhandle/fibonacci.uplc
   - The program should compute the scenario’s required result deterministically within budget.

1. Capture metrics automatically

   Use the automated measurement command to generate schema-compliant metrics.json files.

   - Modes:

     - Measure a single file and write to a specific output:

       ```zsh
       cape submission measure -i submissions/fibonacci/MyCompiler_1.0.0_myhandle/fibonacci.uplc -o submissions/fibonacci/MyCompiler_1.0.0_myhandle/metrics.json
       ```

     - Measure all .uplc files under a path (e.g., your submission directory):

       ```zsh
       cape submission measure submissions/fibonacci/MyCompiler_1.0.0_myhandle
       # or, from inside the submission directory
       cape submission measure .
       ```

     - Measure every submission under submissions/:

       ```zsh
       cape submission measure --all
       ```

   - What it does automatically:

     - Measures CPU units, memory units, script size, and term size for your .uplc file(s)
     - Generates or updates a `metrics.json` with scenario, measurements, evaluator, and timestamp
     - Keeps your existing `notes` and `version` if present; otherwise fills sensible defaults
     - Works for a single file, a directory, or all submissions with `--all`
     - Produces output that validates against `submissions/TEMPLATE/metrics.schema.json`

   - Resulting file example:

     ```json
     {
       "scenario": "fibonacci",
       "version": "1.0.0",
       "measurements": {
         "cpu_units": 185916,
         "memory_units": 592,
         "script_size_bytes": 1234,
         "term_size": 45
       },
       "execution_environment": {
         "evaluator": "plutus-core-executable-1.52.0.0"
       },
       "timestamp": "2025-01-15T00:00:00Z",
       "notes": "Optional notes."
     }
     ```

1. Provide metadata

   Create `metadata.json` according to `submissions/TEMPLATE/metadata.schema.json` (see also `metadata-template.json`).

   ```json
   {
     "compiler": {
       "name": "MyCompiler",
       "version": "1.0.0"
     },
     "compilation_config": {
       "target": "uplc",
       "optimization_level": "O2",
       "flags": []
     },
     "contributor": {
       "name": "myhandle"
     },
     "submission": {
       "date": "2025-01-15T00:00:00Z",
       "source_available": false,
       "implementation_notes": "Brief explanation of approach."
     }
   }
   ```

1. Validate

   ```zsh
   # From the submission directory
   cape submission validate

   # Or from project root
   cape submission validate submissions/fibonacci/MyCompiler_1.0.0_myhandle/

   # Validate specific files
   cape submission validate --single metrics.json
   cape submission validate --single metadata.json
   ```

1. Document

   - Add notes to README.md inside your submission folder (implementation choices, optimizations, caveats).

---

## Metrics Explained

| Metric       | Description                         | Measurement        |
| ------------ | ----------------------------------- | ------------------ |
| CPU Units    | Computational cost for execution    | CEK machine steps  |
| Memory Units | Memory consumption during execution | CEK machine memory |
| Script Size  | Size of serialized UPLC script      | Bytes              |
| Term Size    | Size of the UPLC term               | AST nodes          |

---

## Project Structure

```text
UPLC-CAPE/
├── scenarios/                    # Benchmark specifications
│   ├── TEMPLATE/                 # Template for new scenarios
│   └── fibonacci.md
├── submissions/                  # Compiler submissions (per scenario)
│   ├── TEMPLATE/                 # Templates and schemas
│   │   ├── metadata.schema.json
│   │   ├── metadata-template.json
│   │   ├── metrics.schema.json
│   │   └── metrics-template.json
│   └── fibonacci/
│       └── MyCompiler_1.0.0_handle/
├── scripts/                      # Project CLI tooling
│   ├── cape.sh                   # Main CLI
│   └── cape-subcommands/         # Command implementations
├── plinth/                       # Haskell reference implementations and examples
├── measure/                      # Measurement tooling
├── report/                       # Generated HTML reports and assets
├── doc/                          # Documentation
│   ├── domain-model.md
│   └── adr/
└── README.md
```

---

## Resources

- [Log4brains](https://github.com/thomvaill/log4brains)
- [Architecture Decision Records](http://thinkrelevance.com/blog/2011/11/15/documenting-architecture-decisions)

---

## Version and Tooling Requirements

- Development environment: Nix shell (`nix develop`) with optional direnv (`direnv allow`).
- GHC: 9.6.6 (provided in Nix shell).
- Plutus Core target: 1.1.0.
  - Use `plcVersion110` (for code in plinth-workspace).
- Package baselines (CHaP):
  - plutus-core >= 1.52.0.0
  - plutus-tx >= 1.52.0.0
  - plutus-ledger-api >= 1.52.0.0
  - plutus-tx-plugin >= 1.52.0.0

---

## Development

Enter environment:

```zsh
nix develop
# or
direnv allow
```

Common tools:

- cape … (project CLI)
- cabal build (in plinth-workspace and related components)
- treefmt (format all files)
- fourmolu (Haskell formatting)
- adr (Architecture Decision Records)
- mmdc -i file.mmd (diagram generation, if available)

---

## Documentation (ADRs)

ADRs document important design decisions (managed with Log4brains).

Helpful commands:

```zsh
adr new "Decision Title"
adr preview
adr build
adr help
```

---

## Contributing

We welcome contributions from compiler authors, benchmark designers, and researchers.

- Add a new benchmark:

  ```zsh
  cape benchmark new my-new-benchmark
  # edit scenarios/my-new-benchmark.md
  ```

- Add a submission:

  ```zsh
  cape submission new existing-benchmark MyCompiler 1.0.0 myhandle
  # fill uplc and json files, then open a PR
  ```

Please read [CONTRIBUTING.md](CONTRIBUTING.md) before opening a PR.

---

## License

Licensed under the Apache License 2.0. See [LICENSE](LICENSE).

---

## Acknowledgments

- Plutus Core team for infrastructure and reference implementations
- Compiler authors and community contributors
